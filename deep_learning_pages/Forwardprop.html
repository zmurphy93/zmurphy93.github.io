<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Zach Murphy</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <!-- Latest compiled and minified CSS -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">
  <!-- jQuery library -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <!-- Popper JS -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
  <!-- Latest compiled JavaScript -->
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>

  <link rel="stylesheet" href="ml.css">

</head>

<body>
  <div class="alg-content">
    <span class="alg-heading">Forwardpropagation</span>
    <hr><br>

    <div class="sub-heading">Introduction</div>

    <span class="alg-specs">
      Now that we know how to initialize a neural network, as well as how to perform the
      computations that it will perform on data, we can now write the function that will actually
      move data from one end of the neural network to the other. This process is called
      <strong>forwardpropagation</strong>.
    </span><br><br>

    <div class="sub-heading">Defining the Forwardpropagation Process</div>
    <span class="alg-specs">
      The first part of forward propagation is to apply a linear transformation on incoming data.
      This part takes place in the "synapses" of the network and involves multiplying the data
      by the unique weight that is associated with the destination neuron in the hidden layer. An
      additional vector of biases is also added to the data.
    </span>
    <br><br>

    <span class="alg-specs">
      Upon undergoing the linear transformation, our data arrives at the neuron. where the activation function
      performs a nonlinear transformation on the data.
    </span>
    <br><br>

    <span class="alg-specs">
      This process repeats for each subsequent hidden layer in the neural network. As a result,
      we can express this in a for loop.
    </span>
    <br><br>
    <span class="alg-specs">
      Our final layer will perform one last pair of transformations in order to arrive at a prediction.
      This layer sometimes has an activation function that differs from those within the hidden layer, and we will
      use of this here.
    </span>
    <br><br>
    <span class="alg-specs">
      The last thing we need to do is to write a cost function. This function will be used to evaluate the predictions of
      our network. A variety of cost functions exist, but one that is most commonly used is called <strong>cross-entropy.</strong>
      Mathematically, this is expressed as
    </span>
    <br><br>

</body>

</html>
